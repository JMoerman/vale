include "../../../../arch/x64/X64.Vale.InsBasic.vaf"
include "../../../../arch/x64/X64.Vale.InsMem.vaf"
include "../../../../arch/x64/X64.Vale.InsVector.vaf"
include "../../../../arch/x64/X64.Vale.InsAes.vaf"

module X64.AESCTR

#verbatim{:interface}{:implementation}
//open Opaque_s
open Words_s
open Types_s
open FStar.Seq
open AES_s
open X64.Machine_s
open X64.Memory_i
open X64.Vale.State_i
open X64.Vale.Decls_i
open X64.Vale.InsBasic
open X64.Vale.InsMem
open X64.Vale.InsVector
open X64.Vale.InsAes
open X64.Vale.QuickCode_i
open X64.Vale.QuickCodes_i
open Types_i
open AES_helpers_i
open X64.Poly1305.Math_i    // For lemma_poly_bits64()
open GCM_helpers_i
//open Types_i
//open AES256_helpers_i
#endverbatim

#verbatim{:interface}

let aes_reqs (alg:algorithm) (key:aes_key_LE(alg)) (round_keys:seq(quad32)) (keys_b:buffer128)
             (key_ptr:nat64) (mem:mem) (memTaint:memtaint) : Type0 =
    alg == AES_128 /\
    length(round_keys) == nr(alg) + 1 /\
    round_keys == key_to_round_keys_LE alg key /\
    key_ptr == buffer_addr keys_b mem /\
    validSrcAddrs128 mem key_ptr keys_b (nr alg + 1) memTaint Secret /\
    buffer128_as_seq mem keys_b == round_keys

#endverbatim


#reset-options "--z3rlimit 30"
// Intel's LOOP_4
procedure {:quick} aes_ctr_loop_body(
    inline alg:algorithm,
    ghost plain_b:buffer128,
    ghost out_b:buffer128,

    ghost key:aes_key_LE(alg),
    ghost round_keys:seq(quad32),
    ghost keys_b:buffer128
    )
    lets 
        plain_ptr @= rdi; num_quad_blocks @= r8; out_ptr @= rsi; key_ptr @= r9;
        ctr2_1 @= xmm1; ctr3_4 @= xmm2; iv @= xmm0; mask @= xmm3; four @= xmm4;
        tmp_xmm @= xmm5;
    requires
        // There's at least one block of four left to do
        num_quad_blocks > 0;

        // Valid ptrs and buffers
        validSrcAddrs128(mem, plain_ptr,  plain_b, num_quad_blocks * 64, memTaint, Secret);
        validDstAddrs128(mem, out_ptr,    out_b,   num_quad_blocks * 64, memTaint, Secret);
        plain_ptr + num_quad_blocks * 64 < pow2_64;
        out_ptr   + num_quad_blocks * 64 < pow2_64;
        buffer_length(plain_b)  <= buffer_length(out_b);

        // XMM constants are correct
        mask == Mkfour(0x04050607, 0x00010203, 0x0C0D0E0F, 0x08090A0B);
        four == Mkfour(4, 0, 4, 0);

        // AES reqs
        aes_reqs(alg, key, round_keys, keys_b, key_ptr, mem, memTaint);
    reads key_ptr; iv; mask; four; memTaint;
    modifies r10; xmm1; xmm2; xmm7; xmm8; xmm9; xmm10; xmm11; xmm12; xmm13; xmm14; 
             tmp_xmm; plain_ptr; num_quad_blocks; out_ptr; efl; mem;
    ensures 
        true;
{
    Mov128(xmm11, iv);        // xmm11==12==13==14 == LSB: nonce | ivec_lo | ivec_hi | 0
    Mov128(xmm12, iv);
    Mov128(xmm13, iv);
    Mov128(xmm14, iv);

    Shufpd(xmm11, ctr2_1, 2); // xmm11 = LSB: nonce | ivec_lo | ivec_hi | bswap(1)
    Shufpd(xmm12, ctr2_1, 0); // xmm12 = LSB: nonce | ivec_lo | ivec_hi | bswap(2)
    Shufpd(xmm13, ctr3_4, 2); // xmm13 = LSB: nonce | ivec_lo | ivec_hi | bswap(3)
    Shufpd(xmm14, ctr3_4, 0); // xmm14 = LSB: nonce | ivec_lo | ivec_hi | bswap(4)
    ghost var ctr1 := xmm11;

    Pshufb64(xmm1, mask);     // xmm1 = LSB: 2 | bswap(ivec_hi) | 1 | bswap(ivec_hi)
    Pshufb64(xmm2, mask);     // xmm2 = LSB: 4 | bswap(ivec_hi) | 3 | bswap(ivec_hi)

    // Load the next four round key blocks
    Load128_buffer(xmm8,  key_ptr,  0, Secret, keys_b, 0);
    Load128_buffer(xmm9,  key_ptr, 16, Secret, keys_b, 1);
    Load128_buffer(xmm10, key_ptr, 32, Secret, keys_b, 2);
    Load128_buffer(xmm7,  key_ptr, 48, Secret, keys_b, 3);
    assert{:quick_type} length(round_keys) == 11;
    assert xmm8  == index(round_keys, 0);
    assert xmm9  == index(round_keys, 1);
    assert xmm10 == index(round_keys, 2);
    assert xmm7  == index(round_keys, 3);

    // Pre-emptively increment our counters
    Paddd(xmm1, four);        // xmm1 = LSB: 6 | bswap(ivec_hi) | 5 | bswap(ivec_hi)
    Paddd(xmm2, four);        // xmm2 = LSB: 8 | bswap(ivec_hi) | 7 | bswap(ivec_hi)

    // Begin AES block encrypt by xor'ing four blocks of counters with the key block 0 
    Pxor(xmm11, xmm8);
    Pxor(xmm12, xmm8);
    Pxor(xmm13, xmm8);
    Pxor(xmm14, xmm8);

    Pshufb64(xmm1, mask);      // xmm1 = LSB: ivec_hi | bswap(6) | ivec_hi | bswap(5)
    Pshufb64(xmm2, mask);      // xmm2 = LSB: ivec_hi | bswap(8) | ivec_hi | bswap(7)

    // Compute one AES round with key block 1 for all four counters
    AESNI_enc(xmm11, xmm9);
    AESNI_enc(xmm12, xmm9);
    AESNI_enc(xmm13, xmm9);
    AESNI_enc(xmm14, xmm9);
    
    // Compute one AES round with key block 2 for all four counters
    AESNI_enc(xmm11, xmm10);
    AESNI_enc(xmm12, xmm10);
    AESNI_enc(xmm13, xmm10);
    AESNI_enc(xmm14, xmm10);

    // Compute one AES round with key block 3 for all four counters
    AESNI_enc(xmm11, xmm7);
    AESNI_enc(xmm12, xmm7);
    AESNI_enc(xmm13, xmm7);
    AESNI_enc(xmm14, xmm7);

    // Load the next four round key blocks
    Load128_buffer(xmm8,  key_ptr,  64, Secret, keys_b, 4);
    Load128_buffer(xmm9,  key_ptr,  80, Secret, keys_b, 5);
    Load128_buffer(xmm10, key_ptr,  96, Secret, keys_b, 6);
    Load128_buffer(xmm7,  key_ptr, 112, Secret, keys_b, 7);
    assert xmm8  == index(round_keys, 4);
    assert xmm9  == index(round_keys, 5);
    assert xmm10 == index(round_keys, 6);
    assert xmm7  == index(round_keys, 7);

    // Do another 4 AES rounds for each of the four counters = 7 rounds total (not counting xor step)
    AESNI_enc(xmm11, xmm8);
    AESNI_enc(xmm12, xmm8);
    AESNI_enc(xmm13, xmm8);
    AESNI_enc(xmm14, xmm8);
    
    AESNI_enc(xmm11, xmm9);
    AESNI_enc(xmm12, xmm9);
    AESNI_enc(xmm13, xmm9);
    AESNI_enc(xmm14, xmm9);
    
    AESNI_enc(xmm11, xmm10);
    AESNI_enc(xmm12, xmm10);
    AESNI_enc(xmm13, xmm10);
    AESNI_enc(xmm14, xmm10);

    AESNI_enc(xmm11, xmm7);
    AESNI_enc(xmm12, xmm7);
    AESNI_enc(xmm13, xmm7);
    AESNI_enc(xmm14, xmm7);

    // Load the next three round key blocks
    Load128_buffer(xmm8,  key_ptr, 128, Secret, keys_b, 8);
    Load128_buffer(xmm9,  key_ptr, 144, Secret, keys_b, 9);
    Load128_buffer(xmm10, key_ptr, 160, Secret, keys_b, 10);
    assert xmm8  == index(round_keys, 8);
    assert xmm9  == index(round_keys, 9);
    assert xmm10 == index(round_keys, 10);

    // Do another 2 AES rounds for each of the four counters = 9 rounds total (not counting xor step)
    AESNI_enc(xmm11, xmm8);
    AESNI_enc(xmm12, xmm8);
    AESNI_enc(xmm13, xmm8);
    AESNI_enc(xmm14, xmm8);
    
    AESNI_enc(xmm11, xmm9);
    AESNI_enc(xmm12, xmm9);
    AESNI_enc(xmm13, xmm9);
    AESNI_enc(xmm14, xmm9);

    // TODO: AES_192 and AES_256 would do a few more rounds here

    // Intel's LAST_4

    // Compute the last AES round for each of the four counters
    AESNI_enc_last(xmm11, xmm10);
    AESNI_enc_last(xmm12, xmm10);
    AESNI_enc_last(xmm13, xmm10);
    AESNI_enc_last(xmm14, xmm10);
    
    commute_sub_bytes_shift_rows_forall();
    assert xmm11 == aes_encrypt_LE(alg, key, ctr1);

    // Xor the plaintext with the encrypted counter
    // TODO: Intel does this using XMM operands
    Load128_buffer(tmp_xmm, plain_ptr,  0, Secret, plain_b, 0);
    Pxor(xmm11, tmp_xmm);
    Load128_buffer(tmp_xmm, plain_ptr, 16, Secret, plain_b, 1);
    Pxor(xmm12, tmp_xmm);
    Load128_buffer(tmp_xmm, plain_ptr, 32, Secret, plain_b, 2);
    Pxor(xmm13, tmp_xmm);
    Load128_buffer(tmp_xmm, plain_ptr, 48, Secret, plain_b, 3);
    Pxor(xmm14, tmp_xmm);

    // Store the cipher text in output
    Store128_buffer(out_ptr, xmm11,  0, Secret, out_b, 0);
    Store128_buffer(out_ptr, xmm12, 16, Secret, out_b, 1);
    Store128_buffer(out_ptr, xmm13, 32, Secret, out_b, 2);
    Store128_buffer(out_ptr, xmm14, 48, Secret, out_b, 3);

    Sub64(r8, 1);
    Add64(plain_ptr, 64);
    Add64(out_ptr, 64);
}

// TODO: Add XMM memory operands, so we can avoid using xmm3 to hold constants
/*
procedure {:quick} aes_ctr_inner(
    inline alg:algorithm,
    ghost plain_b:buffer128,
    ghost out_b:buffer128,

    ghost key:aes_key_LE(alg),
    ghost round_keys:seq(quad32),
    ghost keys_b:buffer128
    )
    lets 
        plain_ptr @= rdi; num_bytes @= r8; out_ptr @= rsi; key_ptr @= r9;
        ivec @= rdx; nonce @= rcx; tmp_xmm @= xmm5; // r12 = num_rounds (nr)
    requires
        validSrcAddrs128(mem, plain_ptr,  plain_b,  bytes_to_quad_size(num_bytes), memTaint, Secret);
        validDstAddrs128(mem, out_ptr, out_b, bytes_to_quad_size(num_bytes), memTaint, Secret);
        plain_ptr  + 16 * bytes_to_quad_size(num_bytes) < pow2_64;
        out_ptr + 16 * bytes_to_quad_size(num_bytes) < pow2_64;
        buffer_length(plain_b)  <= buffer_length(out_b);
        num_bytes < pow2_32;
        nonce < pow2_32;

        // AES reqs
        aes_reqs(alg, key, round_keys, keys_b, key_ptr, mem, memTaint);
    reads key_ptr; ivec; nonce; memTaint;
    modifies rax; r10; xmm0; xmm1; xmm2; xmm3; plain_ptr; num_bytes; out_ptr; tmp_xmm; efl; mem;
    ensures 
        true;
{
    lemma_poly_bits64();

    Mov64(r10, r8);
    Shr64(r8, 4);   // r8 := num_bytes / 16 == num_blocks
    And64(r10, 15); // r10 := num_bytes % 16 (note that Intel uses Shl64(r10, 60))
    assert r8 == old(num_bytes) / 16;
    assert r10 == old(num_bytes) % 16;
    if (r10 > 0) {
        Add64(r8, 1);  // Add one to account for trailing blocks
    }
    ghost var num_blocks := r8;

    // Intel: NO_PARTS_4
    Mov64(r10, r8);
    And64(r10, 3); // r10 := num_blocks % 4 (note that Intel uses Shl64(r10, 60); Shr64(r10,60))
    assert r10 == num_blocks % 4;

    // Initialize masks and counters
    Pinsrq(xmm0, nonce, 1);     // Put ivec in the upper 64 bits of xmm0
    Pinsrd(xmm0, rcx, 1);       // Put nonce in the upper 32 bits of the lower 64 bits of xmm0
    Psrldq(xmm0, 4);
    Mov128(xmm2, xmm0);             // Copy xmm0 into xmm2
    InitPshufbDupMask(xmm3, rax);   // xmm3 := dup_mask
    PshufbDup(xmm2, xmm3);          // LSB: 0 | bswap(ivec_hi) | 0 | bswap(ivec_hi) 
    PinsrdImm(xmm3, 2, 0, rax);
    PinsrdImm(xmm3, 1, 2, rax);     // xmm3 := TWO_N_ONE
    Paddd(xmm2, xmm3);
    Mov128(xmm1, xmm2);
    PinsrdImm(xmm3, 2, 2, rax);     // xmm3 := TWO_N_TWO
    Paddd(xmm2, xmm3);
    InitPshufb64Mask(xmm3, rax);
    Pshufb64(xmm1, xmm3);           // xmm1 == LSB: ivec_hi | bswap(2) | ivec_hi | bswap(1)
    Pshufb64(xmm2, xmm3);           // xmm2 == LSB: ivec_hi | bswap(4) | ivec_hi | bswap(3)

    Shr64(r8, 2);
    assert r8 == num_blocks / 4;
/*
    if (r8 > 0) {
//        Sub64(plain_ptr, 64);     // TODO: We would need to prove that plain_ptr >= 64...
//        Sub64(out_ptr, 64);
        //aes_ctr_loop();
    }
    
    // Intel: REMAINDER_4
    if (r10 != 0) {
        Shufpd(xmm0, xmm1, 2);      // xmm0 := LSB: 4 | nonce | ivec + 1
        
        while (r10 > 0) {
            // BP: This isn't doing anything particularly interesting w.r.t. AES
            //     We can probably replace almost all of this with AESEncryptBlock

            // Intel: IN_LOOP_4
            Mov128(xmm11, xmm0);    // Save a copy of current counter value (xmm0) in xmm11
            Pshufb64(xmm0, mask);

            // Load next four key blocks (TODO: Intel does this via XMM memory operands)
            Load128_buffer(xmm8,  key_ptr,  0, Secret, keys_b, 0);
            Load128_buffer(xmm9,  key_ptr, 16, Secret, keys_b, 1);
            Load128_buffer(xmm10, key_ptr, 32, Secret, keys_b, 2);
            Load128_buffer(xmm7,  key_ptr, 48, Secret, keys_b, 3);

            Pxor(xmm11, xmm8);   // Start encrypting xmm11
            Paddd(xmm0, one);    // Increment the counter by 1
            AESNI_enc(xmm11, xmm9);   // Compute AES round 1
            AESNI_enc(xmm11, xmm10);  // Compute AES round 2
            Pshufb64(xmm0, mask);     // Bswap the upper and lower 64-bit slots of xmm0 
            AESNI_enc(xmm11, xmm7);   // Compute AES round 3

            // Load next four key blocks (TODO: Intel does this via XMM memory operands)
            Load128_buffer(xmm8,  key_ptr,  64, Secret, keys_b, 4);
            Load128_buffer(xmm9,  key_ptr,  80, Secret, keys_b, 5);
            Load128_buffer(xmm10, key_ptr,  96, Secret, keys_b, 6);
            Load128_buffer(xmm7,  key_ptr, 112, Secret, keys_b, 7);

            AESNI_enc(xmm11, xmm8);   // Compute AES round 4
            AESNI_enc(xmm11, xmm9);   // Compute AES round 5
            AESNI_enc(xmm11, xmm10);  // Compute AES round 6
            AESNI_enc(xmm11, xmm7);   // Compute AES round 7

            // Load the next three round key blocks
            Load128_buffer(xmm8,  key_ptr, 128, Secret, keys_b, 7);
            Load128_buffer(xmm9,  key_ptr, 144, Secret, keys_b, 8);
            Load128_buffer(xmm10, key_ptr, 160, Secret, keys_b, 9);

            AESNI_enc(xmm11, xmm8);   // Compute AES round 8
            AESNI_enc(xmm11, xmm9);   // Compute AES round 9

            // TODO: AES_192 and AES_256 would do more work here

            // Intel: IN_LAST_4
            AESNI_enc_last(xmm11, xmm10);   // Compute the final AES round
            // Load the plain text
            // TODO: Intel does this using XMM operands
            Load128_buffer(tmp_xmm, plain_ptr,  0, Secret, plain_b, 0); // TODO: Fix indexing
            Pxor(xmm11, tmp_xmm);              
            // Store the cipher text in output
            Store128_buffer(out_ptr, xmm11,  0, Secret, out_b, 0);  // TODO: Fix indexing
            Add64(plain_ptr, 16);
            Add64(out_ptr, 16);
            Sub64(r10, 1);
        }
    }
*/
}
*/
